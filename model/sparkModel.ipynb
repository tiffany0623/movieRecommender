{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e3dabcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import getpass\n",
    "\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "builder. \\\n",
    "config('spark.ui.port','0'). \\\n",
    "config(\"spark.sql.warehouse.dir\", f\"/user/{username}/warehouse\"). \\\n",
    "enableHiveSupport(). \\\n",
    "master('yarn'). \\\n",
    "getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf06d4d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://g01.itversity.com:39613\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f69753f4518>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1e3237",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d21b68",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f023b68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import csv\n",
    "import os\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.ml.recommendation import ALS,ALSModel\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import TrainValidationSplit, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7befd3fb",
   "metadata": {},
   "source": [
    "### Schema Writing & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32be4df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_ratings = StructType([\n",
    "    StructField(\"item_id\", IntegerType(), False),\n",
    "    StructField(\"user_id\", IntegerType(), False),\n",
    "    StructField(\"rating\", FloatType(), False)])\n",
    "\n",
    "schema_items = StructType([\n",
    "    StructField(\"item_id\", IntegerType(), False),\n",
    "    StructField(\"title\", StringType(), False)])\n",
    "\n",
    "# schema_reviews = StructType([\n",
    "#     StructField(\"item_id\", IntegerType(), False),\n",
    "#     StructField(\"txt\", StringType(), False)])\n",
    "\n",
    "\n",
    "ratings_df = spark.read.json(\"/user/itv009301/movieDataset/ratings.json\", schema=schema_ratings)\n",
    "metadata_df = spark.read.json(\"/user/itv009301/movieDataset/metadata.json\", schema=schema_items)\n",
    "# reviews_df = spark.read.json(\"/user/itv009301/movieDataset/reviews.json\", schema=schema_reviews)\n",
    "# metadata_updated_df = spark.read.option('inferschema', 'true').json(\"/user/itv009301/movieDataset/metadata_updated.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d8e0ad",
   "metadata": {},
   "source": [
    "### Filter ratings dataframe\n",
    ">Filter records which are present in both ratings.json and metadata.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12263d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_ratings_df = metadata_df.alias(\"m\").join(\n",
    "    ratings_df.alias(\"r\"),\n",
    "    metadata_df[\"item_id\"] == ratings_df[\"item_id\"],\n",
    "    \"inner\").select(\"r.*\")\n",
    "# print(\"Number of records before filtering: \",ratings_df.count())\n",
    "# print(\"\\nNumber of records after filtering: \",filtered_ratings_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff8c736",
   "metadata": {},
   "source": [
    "### Take out test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ce69af",
   "metadata": {},
   "source": [
    ">When training recommendation algorithms, the traditional random split of data into training and testing sets may not be suitable. In the context of recommendation systems, we need to handle the unique challenge of ensuring that all users are present in both the training and testing datasets.\n",
    "To address this issue, a common approach is to perform a user-level dataset split. Instead of randomly dividing the data at the individual data point level, we mask a certain percentage of items for each user and use these masked items as the testing set. Specifically, we randomly hide a portion (e.g., 20%) of the items that each user has interacted with in the training set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f35ab9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_ratings_df = filtered_ratings_df.withColumn(\"num_items\", expr(\"count(*) over (partition by user_id)\"))\n",
    "\n",
    "# # 20% of items will be masked\n",
    "# percent_items_to_mask = 0.2\n",
    "\n",
    "# # Determine the number of items to mask for each user\n",
    "# filtered_ratings_df = filtered_ratings_df.withColumn(\"num_items_to_mask\", \\\n",
    "#                                                      (col(\"num_items\") * percent_items_to_mask).cast(\"int\"))\n",
    "# # Masks items for each user\n",
    "# filtered_ratings_df = filtered_ratings_df.withColumn(\"item_rank\", \\\n",
    "#                                           rank().over(Window.partitionBy(\"user_id\").orderBy(col(\"item_id\").desc())))\n",
    "\n",
    "training_df, testing_df = filtered_ratings_df.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a159e221",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf70b57",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2087c687",
   "metadata": {},
   "source": [
    "#### Model Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b73e346b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = 200\n",
    "numIterations = 30\n",
    "regParameter = 0.05\n",
    "nonNegativity = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80254a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training in process.....\n"
     ]
    }
   ],
   "source": [
    "print('Model training in process.....')\n",
    "t0 = time()\n",
    "\n",
    "# Initialize the ALS model\n",
    "als = ALS(maxIter=numIterations \\\n",
    "          , rank=rank \\\n",
    "          , regParam=regParameter\\\n",
    "          , userCol=\"user_id\" \\\n",
    "          , itemCol=\"item_id\"\n",
    "          , ratingCol=\"rating\"\n",
    "          , coldStartStrategy=\"drop\"\n",
    "          , nonnegative=nonNegativity)\n",
    "\n",
    "# Fit the model\n",
    "model = als.fit(training_df)\n",
    "\n",
    "\n",
    "print(\"Model trained in seconds\", time()-t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd5cbd5",
   "metadata": {},
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a57560",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelPath = f\"/user/itv009301/model/rank_{rank}_lambda_{regParameter}_model\"\n",
    "model.save(modelPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e03600",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949caef8",
   "metadata": {},
   "source": [
    ">User-level split allows us to ensure that the testing set contains users, which were part of training process, which is critical for evaluating the performance of the recommendation system accurately. By using metrics such as precision and NDCG on the masked items, we can compare the predicted recommendations with the actual hidden items to assess the quality and effectiveness of the recommendation model. This method provides a more realistic evaluation of the recommendation systemâ€™s performance in a scenario where some user-item interactions are unknown during training but need to be predicted during testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee9f5368",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o288.evaluate.\n: org.apache.spark.SparkException: Job aborted due to stage failure: ShuffleMapStage 31 (rdd at RegressionEvaluator.scala:125) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.FetchFailedException: java.lang.IllegalArgumentException: Unknown message type: 9 \tat org.apache.spark.network.shuffle.protocol.BlockTransferMessage$Decoder.fromByteBuffer(BlockTransferMessage.java:71) \tat org.apache.spark.network.shuffle.ExternalShuffleBlockHandler.receive(ExternalShuffleBlockHandler.java:80) \tat org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:180) \tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:103) \tat org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) \tat org.spark_project.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) \tat org.spark_project.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) \tat org.spark_project.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) \tat org.spark_project.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) \tat java.lang.Thread.run(Thread.java:748)  \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:770) \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:685) \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70) \tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29) \tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484) \tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490) \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458) \tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31) \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37) \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458) \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.sort_addToSorter_0$(Unknown Source) \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source) \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) \tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755) \tat org.apache.spark.sql.execution.RowIteratorFromScala.advanceNext(RowIterator.scala:83) \tat org.apache.spark.sql.execution.joins.SortMergeJoinScanner.advancedStreamed(SortMergeJoinExec.scala:800) \tat org.apache.spark.sql.execution.joins.SortMergeJoinScanner.findNextOuterJoinRows(SortMergeJoinExec.scala:759) \tat org.apache.spark.sql.execution.joins.OneSideOuterIterator.advanceStream(SortMergeJoinExec.scala:925) \tat org.apache.spark.sql.execution.joins.OneSideOuterIterator.advanceNext(SortMergeJoinExec.scala:961) \tat org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:68) \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source) \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) \tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755) \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458) \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132) \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59) \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99) \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52) \tat org.apache.spark.scheduler.Task.run(Task.scala:131) \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497) \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500) \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) \tat java.lang.Thread.run(Thread.java:748) Caused by: java.lang.RuntimeException: java.lang.IllegalArgumentException: Unknown message type: 9 \tat org.apache.spark.network.shuffle.protocol.BlockTransferMessage$Decoder.fromByteBuffer(BlockTransferMessage.java:71) \tat org.apache.spark.network.shuffle.ExternalShuffleBlockHandler.receive(ExternalShuffleBlockHandler.java:80) \tat org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:180) \tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:103) \tat org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) \tat org.spark_project.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) \tat org.spark_project.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) \tat org.spark_project.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) \tat org.spark_project.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) \tat java.lang.Thread.run(Thread.java:748)  \tat org.apache.spark.network.client.TransportResponseHandler.handle(TransportResponseHandler.java:208) \tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:142) \tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53) \tat io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) \tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163) \tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714) \tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650) \tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576) \tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) \tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) \tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) \tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) \t... 1 more \n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1768)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2442)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2291)\n\tat org.apache.spark.rdd.RDD.$anonfun$fold$1(RDD.scala:1183)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1177)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$1(RDD.scala:1246)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1222)\n\tat org.apache.spark.mllib.stat.Statistics$.colStats(Statistics.scala:58)\n\tat org.apache.spark.mllib.evaluation.RegressionMetrics.summary$lzycompute(RegressionMetrics.scala:70)\n\tat org.apache.spark.mllib.evaluation.RegressionMetrics.summary(RegressionMetrics.scala:62)\n\tat org.apache.spark.mllib.evaluation.RegressionMetrics.SSerr$lzycompute(RegressionMetrics.scala:74)\n\tat org.apache.spark.mllib.evaluation.RegressionMetrics.SSerr(RegressionMetrics.scala:74)\n\tat org.apache.spark.mllib.evaluation.RegressionMetrics.meanSquaredError(RegressionMetrics.scala:106)\n\tat org.apache.spark.mllib.evaluation.RegressionMetrics.rootMeanSquaredError(RegressionMetrics.scala:115)\n\tat org.apache.spark.ml.evaluation.RegressionEvaluator.evaluate(RegressionEvaluator.scala:101)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-42c6a192d4a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mevaluator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRegressionEvaluator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetricName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"rmse\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabelCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"rating\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictionCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"prediction\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mrmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"RMSE without Grid Search: {rank}, {numIterations}, {regParameter},{nonNegativity}, {rmse}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/evaluation.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/evaluation.py\u001b[0m in \u001b[0;36m_evaluate\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \"\"\"\n\u001b[1;32m    119\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0misLargerBetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o288.evaluate.\n: org.apache.spark.SparkException: Job aborted due to stage failure: ShuffleMapStage 31 (rdd at RegressionEvaluator.scala:125) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.FetchFailedException: java.lang.IllegalArgumentException: Unknown message type: 9 \tat org.apache.spark.network.shuffle.protocol.BlockTransferMessage$Decoder.fromByteBuffer(BlockTransferMessage.java:71) \tat org.apache.spark.network.shuffle.ExternalShuffleBlockHandler.receive(ExternalShuffleBlockHandler.java:80) \tat org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:180) \tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:103) \tat org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) \tat org.spark_project.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) \tat org.spark_project.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) \tat org.spark_project.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) \tat org.spark_project.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) \tat java.lang.Thread.run(Thread.java:748)  \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:770) \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:685) \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70) \tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29) \tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484) \tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490) \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458) \tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31) \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37) \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458) \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.sort_addToSorter_0$(Unknown Source) \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source) \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) \tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755) \tat org.apache.spark.sql.execution.RowIteratorFromScala.advanceNext(RowIterator.scala:83) \tat org.apache.spark.sql.execution.joins.SortMergeJoinScanner.advancedStreamed(SortMergeJoinExec.scala:800) \tat org.apache.spark.sql.execution.joins.SortMergeJoinScanner.findNextOuterJoinRows(SortMergeJoinExec.scala:759) \tat org.apache.spark.sql.execution.joins.OneSideOuterIterator.advanceStream(SortMergeJoinExec.scala:925) \tat org.apache.spark.sql.execution.joins.OneSideOuterIterator.advanceNext(SortMergeJoinExec.scala:961) \tat org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:68) \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source) \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) \tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755) \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458) \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132) \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59) \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99) \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52) \tat org.apache.spark.scheduler.Task.run(Task.scala:131) \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497) \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500) \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) \tat java.lang.Thread.run(Thread.java:748) Caused by: java.lang.RuntimeException: java.lang.IllegalArgumentException: Unknown message type: 9 \tat org.apache.spark.network.shuffle.protocol.BlockTransferMessage$Decoder.fromByteBuffer(BlockTransferMessage.java:71) \tat org.apache.spark.network.shuffle.ExternalShuffleBlockHandler.receive(ExternalShuffleBlockHandler.java:80) \tat org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:180) \tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:103) \tat org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) \tat org.spark_project.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) \tat org.spark_project.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) \tat org.spark_project.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) \tat org.spark_project.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) \tat java.lang.Thread.run(Thread.java:748)  \tat org.apache.spark.network.client.TransportResponseHandler.handle(TransportResponseHandler.java:208) \tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:142) \tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53) \tat io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) \tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163) \tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714) \tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650) \tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576) \tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) \tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) \tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) \tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) \t... 1 more \n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1768)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2442)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2291)\n\tat org.apache.spark.rdd.RDD.$anonfun$fold$1(RDD.scala:1183)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1177)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$1(RDD.scala:1246)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1222)\n\tat org.apache.spark.mllib.stat.Statistics$.colStats(Statistics.scala:58)\n\tat org.apache.spark.mllib.evaluation.RegressionMetrics.summary$lzycompute(RegressionMetrics.scala:70)\n\tat org.apache.spark.mllib.evaluation.RegressionMetrics.summary(RegressionMetrics.scala:62)\n\tat org.apache.spark.mllib.evaluation.RegressionMetrics.SSerr$lzycompute(RegressionMetrics.scala:74)\n\tat org.apache.spark.mllib.evaluation.RegressionMetrics.SSerr(RegressionMetrics.scala:74)\n\tat org.apache.spark.mllib.evaluation.RegressionMetrics.meanSquaredError(RegressionMetrics.scala:106)\n\tat org.apache.spark.mllib.evaluation.RegressionMetrics.rootMeanSquaredError(RegressionMetrics.scala:115)\n\tat org.apache.spark.ml.evaluation.RegressionEvaluator.evaluate(RegressionEvaluator.scala:101)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    }
   ],
   "source": [
    "# Make recommendation on test dataset (if any users are unseen during training then they will be dropped becasue we have set coldStartStrategy='drop')\n",
    "t0 = time()\n",
    "\n",
    "predictions = model.transform(testing_df)\n",
    "\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "\n",
    "print(f\"RMSE without Grid Search: {rank}, {numIterations}, {regParameter},{nonNegativity}, {rmse}\")\n",
    "\n",
    "print(\"\\nError Calculated in seconds:\", time()-t0)\n",
    "\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb1cda1",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec4991d",
   "metadata": {},
   "source": [
    "### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e8025e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelPath = f\"/user/itv009301/model/rank_{rank}_lambda_{regParameter}_model\"\n",
    "model = ALSModel.load(modelPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d89d4a3",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcaae5c",
   "metadata": {},
   "source": [
    "#### Show Users' historical ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ea201e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "user = 958971\n",
    "df_with_lowest_user_id = filtered_ratings_df.where(filtered_ratings_df.user_id==user).orderBy([\"rating\"], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3452e3ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(item_id=1466, title='Donnie Brasco (1997)', item_id=1466, user_id=958971, rating=3.5),\n",
       " Row(item_id=2542, title='Lock, Stock & Two Smoking Barrels (1998)', item_id=2542, user_id=958971, rating=4.0),\n",
       " Row(item_id=2396, title='Shakespeare in Love (1998)', item_id=2396, user_id=958971, rating=3.5),\n",
       " Row(item_id=2006, title='Mask of Zorro, The (1998)', item_id=2006, user_id=958971, rating=3.5),\n",
       " Row(item_id=4246, title=\"Bridget Jones's Diary (2001)\", item_id=4246, user_id=958971, rating=4.0),\n",
       " Row(item_id=1267, title='Manchurian Candidate, The (1962)', item_id=1267, user_id=958971, rating=4.0),\n",
       " Row(item_id=4995, title='Beautiful Mind, A (2001)', item_id=4995, user_id=958971, rating=4.0),\n",
       " Row(item_id=2770, title='Bowfinger (1999)', item_id=2770, user_id=958971, rating=3.5),\n",
       " Row(item_id=6539, title='Pirates of the Caribbean: The Curse of the Black Pearl (2003)', item_id=6539, user_id=958971, rating=4.0),\n",
       " Row(item_id=2395, title='Rushmore (1998)', item_id=2395, user_id=958971, rating=3.5),\n",
       " Row(item_id=6377, title='Finding Nemo (2003)', item_id=6377, user_id=958971, rating=4.0),\n",
       " Row(item_id=1552, title='Con Air (1997)', item_id=1552, user_id=958971, rating=3.0),\n",
       " Row(item_id=3255, title='League of Their Own, A (1992)', item_id=3255, user_id=958971, rating=4.0),\n",
       " Row(item_id=3081, title='Sleepy Hollow (1999)', item_id=3081, user_id=958971, rating=3.5),\n",
       " Row(item_id=5952, title='Lord of the Rings: The Two Towers, The (2002)', item_id=5952, user_id=958971, rating=5.0),\n",
       " Row(item_id=1408, title='Last of the Mohicans, The (1992)', item_id=1408, user_id=958971, rating=5.0),\n",
       " Row(item_id=2028, title='Saving Private Ryan (1998)', item_id=2028, user_id=958971, rating=5.0),\n",
       " Row(item_id=7153, title='Lord of the Rings: The Return of the King, The (2003)', item_id=7153, user_id=958971, rating=5.0),\n",
       " Row(item_id=2890, title='Three Kings (1999)', item_id=2890, user_id=958971, rating=3.0),\n",
       " Row(item_id=3948, title='Meet the Parents (2000)', item_id=3948, user_id=958971, rating=3.5),\n",
       " Row(item_id=186, title='Nine Months (1995)', item_id=186, user_id=958971, rating=3.0),\n",
       " Row(item_id=2997, title='Being John Malkovich (1999)', item_id=2997, user_id=958971, rating=3.0)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_df.alias(\"m\").join(\n",
    "    df_with_lowest_user_id.alias(\"r\"),\n",
    "    metadata_df[\"item_id\"] == df_with_lowest_user_id[\"item_id\"],\n",
    "    \"inner\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a067e6c3",
   "metadata": {},
   "source": [
    "#### Get Recommendations for selected user_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5ffc50cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+---------+------------------------------------------------------------------------------+\n",
      "|item_id|user_id|rating   |title                                                                         |\n",
      "+-------+-------+---------+------------------------------------------------------------------------------+\n",
      "|188925 |0      |6.7725663|8 Murders a Day (2011)                                                        |\n",
      "|188923 |0      |6.095309 |49 Pulses (2017)                                                              |\n",
      "|165421 |0      |5.501858 |The Red Pill (2016)                                                           |\n",
      "|2609   |0      |4.996947 |King of Masks, The (Bian Lian) (1996)                                         |\n",
      "|4993   |0      |4.991012 |Lord of the Rings: The Fellowship of the Ring, The (2001)                     |\n",
      "|1198   |0      |4.9627814|Raiders of the Lost Ark (Indiana Jones and the Raiders of the Lost Ark) (1981)|\n",
      "|7153   |0      |4.9344115|Lord of the Rings: The Return of the King, The (2003)                         |\n",
      "|5952   |0      |4.927173 |Lord of the Rings: The Two Towers, The (2002)                                 |\n",
      "|260    |0      |4.8812547|Star Wars: Episode IV - A New Hope (1977)                                     |\n",
      "|199340 |0      |4.873112 |Who You Think I Am (2019)                                                     |\n",
      "|182723 |0      |4.858687 |Cosmos: A Spacetime Odissey                                                   |\n",
      "|32444  |0      |4.840101 |Carmen (1983)                                                                 |\n",
      "|71444  |0      |4.823975 |Thirst (Pyaasa) (1957)                                                        |\n",
      "|68835  |0      |4.789694 |Were the World Mine (2008)                                                    |\n",
      "|2675   |0      |4.7717614|Twice Upon a Yesterday (a.k.a. Man with Rain in His Shoes, The) (1998)        |\n",
      "|139863 |0      |4.747421 |Ghost in the Shell Arise - Border 4: Ghost Stands Alone (2014)                |\n",
      "|3245   |0      |4.7382045|I Am Cuba (Soy Cuba/Ya Kuba) (1964)                                           |\n",
      "|170705 |0      |4.719152 |Band of Brothers (2001)                                                       |\n",
      "|1196   |0      |4.7132535|Star Wars: Episode V - The Empire Strikes Back (1980)                         |\n",
      "|92210  |0      |4.7115784|Disappearance of Haruhi Suzumiya, The (Suzumiya Haruhi no shÃ´shitsu) (2010)   |\n",
      "|68358  |0      |4.705344 |Star Trek (2009)                                                              |\n",
      "|2593   |0      |4.701739 |Monster, The (Mostro, Il) (1994)                                              |\n",
      "|59607  |0      |4.7014966|King Corn (2007)                                                              |\n",
      "|1291   |0      |4.700236 |Indiana Jones and the Last Crusade (1989)                                     |\n",
      "|69509  |0      |4.693119 |Vampires, Les (1915)                                                          |\n",
      "|129241 |0      |4.6915803|Tis kakomoiras (1963)                                                         |\n",
      "|168760 |0      |4.690712 |Cosy Dens (1999)                                                              |\n",
      "|78946  |0      |4.690148 |Distant Thunder (Ashani Sanket) (1973)                                        |\n",
      "|1368   |0      |4.6826015|Forbidden Christ, The (Cristo proibito, Il) (1950)                            |\n",
      "|7388   |0      |4.6777406|Brother Sun, Sister Moon (Fratello sole, sorella luna) (1972)                 |\n",
      "+-------+-------+---------+------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_id = 0\n",
    "n_of_recommendations = 30\n",
    "\n",
    "subset_data = [(user_id,)]\n",
    "subset_columns = [\"user_id\"]\n",
    "subset_df = spark.createDataFrame(subset_data, subset_columns)\n",
    "\n",
    "\n",
    "# Generate collaborative filtering recommendations\n",
    "collaborative_recommendations = model.recommendForUserSubset(subset_df, n_of_recommendations)\n",
    "\n",
    "# Explode the array of recommendations to individual rows\n",
    "exploded_df = collaborative_recommendations.select(\"user_id\", explode(\"recommendations\").alias(\"recommendation\"))\n",
    "\n",
    "# Extract item_id and rating columns for the join\n",
    "result_df = exploded_df.select(\n",
    "    \"user_id\",\n",
    "    \"recommendation.item_id\",\n",
    "    \"recommendation.rating\"\n",
    ")\n",
    "\n",
    "# Join with metadata_df on 'item_id'\n",
    "joined_df = result_df.join(metadata_df, on=\"item_id\", how=\"inner\")\n",
    "\n",
    "# Show the final result\n",
    "joined_df.orderBy([\"rating\"], ascending=False).show(30,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bdfe62",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b5ffb013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+---------+---------------------------------------------------------------------------+\n",
      "|item_id|user_id|rating   |title                                                                      |\n",
      "+-------+-------+---------+---------------------------------------------------------------------------+\n",
      "|188925 |552403 |5.992783 |8 Murders a Day (2011)                                                     |\n",
      "|188923 |552403 |5.393504 |49 Pulses (2017)                                                           |\n",
      "|136445 |552403 |4.890968 |George Carlin: Back in Town (1996)                                         |\n",
      "|32649  |552403 |4.886404 |Special Day, A (Giornata particolare, Una) (1977)                          |\n",
      "|26073  |552403 |4.8167443|Human Condition III, The (Ningen no joken III) (1961)                      |\n",
      "|45412  |552403 |4.7861447|Hidden Blade, The (Kakushi ken oni no tsume) (2004)                        |\n",
      "|750    |552403 |4.7584987|Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb (1964)|\n",
      "|6530   |552403 |4.7583723|Tenant, The (Locataire, Le) (1976)                                         |\n",
      "|7327   |552403 |4.723169 |Persona (1966)                                                             |\n",
      "|26048  |552403 |4.6895833|Human Condition II, The (Ningen no joken II) (1959)                        |\n",
      "|1219   |552403 |4.6847763|Psycho (1960)                                                              |\n",
      "|924    |552403 |4.671957 |2001: A Space Odyssey (1968)                                               |\n",
      "|1258   |552403 |4.6709023|Shining, The (1980)                                                        |\n",
      "|171495 |552403 |4.665885 |Cosmos                                                                     |\n",
      "|69241  |552403 |4.656901 |Berlin Alexanderplatz (1980)                                               |\n",
      "|25987  |552403 |4.6448035|Crucified Lovers, The (Chikamatsu monogatari) (1954)                       |\n",
      "|3676   |552403 |4.633274 |Eraserhead (1977)                                                          |\n",
      "|7939   |552403 |4.628878 |Through a Glass Darkly (SÃ¥som i en spegel) (1961)                          |\n",
      "|38473  |552403 |4.627144 |Touch the Sound: A Sound Journey with Evelyn Glennie (2004)                |\n",
      "|114700 |552403 |4.6163025|Tribe, The (Plemya) (2014)                                                 |\n",
      "|872    |552403 |4.6073165|Vive L'Amour (Ai qing wan sui) (1994)                                      |\n",
      "|3245   |552403 |4.595664 |I Am Cuba (Soy Cuba/Ya Kuba) (1964)                                        |\n",
      "|25741  |552403 |4.5901794|Phantom Carriage, The (KÃ¶rkarlen) (1921)                                   |\n",
      "|25954  |552403 |4.589018 |Orpheus (OrphÃ©e) (1950)                                                    |\n",
      "|8620   |552403 |4.5867443|Exterminating Angel, The (Ãngel exterminador, El) (1962)                   |\n",
      "|904    |552403 |4.5853205|Rear Window (1954)                                                         |\n",
      "|2360   |552403 |4.58153  |Celebration, The (Festen) (1998)                                           |\n",
      "|119153 |552403 |4.5792265|Bill Burr: You People Are All the Same (2012)                              |\n",
      "|1232   |552403 |4.578114 |Stalker (1979)                                                             |\n",
      "|3224   |552403 |4.5701957|Woman in the Dunes (Suna no onna) (1964)                                   |\n",
      "+-------+-------+---------+---------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_id = 552403\n",
    "n_of_recommendations = 30\n",
    "\n",
    "subset_data = [(user_id,)]\n",
    "subset_columns = [\"user_id\"]\n",
    "subset_df = spark.createDataFrame(subset_data, subset_columns)\n",
    "\n",
    "\n",
    "# Generate collaborative filtering recommendations\n",
    "collaborative_recommendations = model.recommendForUserSubset(subset_df, n_of_recommendations)\n",
    "\n",
    "# Explode the array of recommendations to individual rows\n",
    "exploded_df = collaborative_recommendations.select(\"user_id\", explode(\"recommendations\").alias(\"recommendation\"))\n",
    "\n",
    "# Extract item_id and rating columns for the join\n",
    "result_df = exploded_df.select(\n",
    "    \"user_id\",\n",
    "    \"recommendation.item_id\",\n",
    "    \"recommendation.rating\"\n",
    ")\n",
    "\n",
    "# Join with metadata_df on 'item_id'\n",
    "joined_df = result_df.join(metadata_df, on=\"item_id\", how=\"inner\")\n",
    "\n",
    "# Show the final result\n",
    "joined_df.orderBy([\"rating\"], ascending=False).show(30,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "43cc9f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+---------+------------------------------------------------------------------------------+\n",
      "|item_id|user_id|rating   |title                                                                         |\n",
      "+-------+-------+---------+------------------------------------------------------------------------------+\n",
      "|188925 |700445 |7.092567 |8 Murders a Day (2011)                                                        |\n",
      "|188923 |700445 |6.3833094|49 Pulses (2017)                                                              |\n",
      "|184299 |700445 |5.2092605|Freedom on My Mind (1994)                                                     |\n",
      "|204304 |700445 |5.0758643|Leningrad: Kolshik (2017)                                                     |\n",
      "|171495 |700445 |5.0580797|Cosmos                                                                        |\n",
      "|166812 |700445 |5.0089564|Seeing Red: Stories of American Communists (1983)                             |\n",
      "|202936 |700445 |4.9813156|ReMoved (2013)                                                                |\n",
      "|156414 |700445 |4.9247465|Life Even Looks Like a Party (2009)                                           |\n",
      "|120821 |700445 |4.861136 |The War at Home (1979)                                                        |\n",
      "|171011 |700445 |4.822198 |Planet Earth II (2016)                                                        |\n",
      "|153038 |700445 |4.804254 |Avetik (1992)                                                                 |\n",
      "|153036 |700445 |4.804254 |AcÃ©phale (1969)                                                               |\n",
      "|154860 |700445 |4.804254 |Mother (2016)                                                                 |\n",
      "|159817 |700445 |4.8016224|Planet Earth (2006)                                                           |\n",
      "|32657  |700445 |4.785368 |Man Who Planted Trees, The (Homme qui plantait des arbres, L') (1987)         |\n",
      "|170705 |700445 |4.76115  |Band of Brothers (2001)                                                       |\n",
      "|104520 |700445 |4.756895 |Crimson Permanent Assurance, The (1983)                                       |\n",
      "|188685 |700445 |4.755588 |Nordexpressen (1992)                                                          |\n",
      "|162864 |700445 |4.746208 |Louis C.K.: One Night Stand (2005)                                            |\n",
      "|200724 |700445 |4.7453127|Along for the Ride (2017)                                                     |\n",
      "|210567 |700445 |4.7434053|Metal and Melancholy (1994)                                                   |\n",
      "|215615 |700445 |4.7433414|Pink Floyd: Pulse (1995)                                                      |\n",
      "|1198   |700445 |4.7342114|Raiders of the Lost Ark (Indiana Jones and the Raiders of the Lost Ark) (1981)|\n",
      "|105250 |700445 |4.7304983|Century of the Self, The (2002)                                               |\n",
      "|92079  |700445 |4.7304487|Nazis: A Warning from History, The (1997)                                     |\n",
      "|106115 |700445 |4.716268 |Story of Science, The (2010)                                                  |\n",
      "|174789 |700445 |4.709277 |Kino-pravda no. 8 (1922)                                                      |\n",
      "|175157 |700445 |4.709277 |Tell Them We Are Rising: The Story of Black Colleges and Universities (2017)  |\n",
      "|173959 |700445 |4.709277 |The Dreamed Ones (2016)                                                       |\n",
      "|193047 |700445 |4.709277 |High Fantasy (2017)                                                           |\n",
      "+-------+-------+---------+------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_id = 700445\n",
    "n_of_recommendations = 30\n",
    "\n",
    "subset_data = [(user_id,)]\n",
    "subset_columns = [\"user_id\"]\n",
    "subset_df = spark.createDataFrame(subset_data, subset_columns)\n",
    "\n",
    "\n",
    "# Generate collaborative filtering recommendations\n",
    "collaborative_recommendations = model.recommendForUserSubset(subset_df, n_of_recommendations)\n",
    "\n",
    "# Explode the array of recommendations to individual rows\n",
    "exploded_df = collaborative_recommendations.select(\"user_id\", explode(\"recommendations\").alias(\"recommendation\"))\n",
    "\n",
    "# Extract item_id and rating columns for the join\n",
    "result_df = exploded_df.select(\n",
    "    \"user_id\",\n",
    "    \"recommendation.item_id\",\n",
    "    \"recommendation.rating\"\n",
    ")\n",
    "\n",
    "# Join with metadata_df on 'item_id'\n",
    "joined_df = result_df.join(metadata_df, on=\"item_id\", how=\"inner\")\n",
    "\n",
    "# Show the final result\n",
    "joined_df.orderBy([\"rating\"], ascending=False).show(30,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f691d6c7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de46a74",
   "metadata": {},
   "source": [
    "## Note:\n",
    "* It is recommeding same movies for every user, users are not new they were part of training, so there is some error with the code that needs to be fixed.\n",
    "* After fixing the issue of overfitting, we are getting some movies rating >5.00, we will remove those recommendations. As they appear for every user and don't align with users' history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1a9724",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
